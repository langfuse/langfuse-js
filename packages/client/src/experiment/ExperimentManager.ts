import { DatasetItem, getGlobalLogger } from "@langfuse/core";
import { startActiveObservation } from "@langfuse/tracing";
import { ProxyTracerProvider, trace } from "@opentelemetry/api";

import { LangfuseClient } from "../LangfuseClient.js";

import {
  ExperimentParams,
  ExperimentResult,
  ExperimentTask,
  ExperimentItem,
  ExperimentItemResult,
  Evaluator,
  Evaluation,
} from "./types.js";

/**
 * Manages the execution and evaluation of experiments on datasets.
 *
 * The ExperimentManager provides a comprehensive framework for running experiments
 * that test models or tasks against datasets, with support for automatic evaluation,
 * scoring.
 *
 * @example Basic experiment usage
 * ```typescript
 * const langfuse = new LangfuseClient();
 *
 * const result = await langfuse.experiment.run({
 *   name: "Capital Cities Test",
 *   description: "Testing model knowledge of world capitals",
 *   data: [
 *     { input: "France", expectedOutput: "Paris" },
 *     { input: "Germany", expectedOutput: "Berlin" }
 *   ],
 *   task: async ({ input }) => {
 *     const response = await openai.chat.completions.create({
 *       model: "gpt-4",
 *       messages: [{ role: "user", content: `What is the capital of ${input}?` }]
 *     });
 *     return response.choices[0].message.content;
 *   },
 *   evaluators: [
 *     async ({ input, output, expectedOutput }) => ({
 *       name: "exact_match",
 *       value: output === expectedOutput ? 1 : 0
 *     })
 *   ]
 * });
 *
 * console.log(await result.prettyPrint());
 * ```
 *
 * @example Using with Langfuse datasets
 * ```typescript
 * const dataset = await langfuse.dataset.get("my-dataset");
 *
 * const result = await dataset.runExperiment({
 *   name: "Model Comparison",
 *   task: myTask,
 *   evaluators: [myEvaluator],
 *   runEvaluators: [averageScoreEvaluator]
 * });
 * ```
 *
 * @public
 */
export class ExperimentManager {
  private langfuseClient: LangfuseClient;

  /**
   * Creates a new ExperimentManager instance.
   *
   * @param params - Configuration object
   * @param params.langfuseClient - The Langfuse client instance for API communication
   * @internal
   */
  constructor(params: { langfuseClient: LangfuseClient }) {
    this.langfuseClient = params.langfuseClient;
  }

  /**
   * Gets the global logger instance for experiment-related logging.
   *
   * @returns The global logger instance
   * @internal
   */
  get logger() {
    return getGlobalLogger();
  }

  /**
   * Executes an experiment by running a task on each data item and evaluating the results.
   *
   * This method orchestrates the complete experiment lifecycle:
   * 1. Executes the task function on each data item with proper tracing
   * 2. Runs item-level evaluators on each task output
   * 3. Executes run-level evaluators on the complete result set
   * 4. Links results to dataset runs (for Langfuse datasets)
   * 5. Stores all scores and traces in Langfuse
   *
   * @param config - The experiment configuration
   * @param config.name - Human-readable name for the experiment
   * @param config.description - Optional description of the experiment's purpose
   * @param config.metadata - Optional metadata to attach to the experiment run
   * @param config.data - Array of data items to process (ExperimentItem[] or DatasetItem[])
   * @param config.task - Function that processes each data item and returns output
   * @param config.evaluators - Optional array of functions to evaluate each item's output
   * @param config.runEvaluators - Optional array of functions to evaluate the entire run
   * @param config.maxConcurrency - Maximum number of concurrent task executions (default: Infinity)
   *
   * @returns Promise that resolves to experiment results including:
   *   - itemResults: Results for each processed data item
   *   - runEvaluations: Results from run-level evaluators
   *   - datasetRunId: ID of the dataset run (if using Langfuse datasets)
   *   - prettyPrint: Function to format and display results
   *
   * @throws {Error} When task execution fails and cannot be handled gracefully
   * @throws {Error} When required evaluators fail critically
   *
   * @example Simple experiment
   * ```typescript
   * const result = await langfuse.experiment.run({
   *   name: "Translation Quality Test",
   *   data: [
   *     { input: "Hello world", expectedOutput: "Hola mundo" },
   *     { input: "Good morning", expectedOutput: "Buenos dÃ­as" }
   *   ],
   *   task: async ({ input }) => translateText(input, 'es'),
   *   evaluators: [
   *     async ({ output, expectedOutput }) => ({
   *       name: "bleu_score",
   *       value: calculateBleuScore(output, expectedOutput)
   *     })
   *   ]
   * });
   * ```
   *
   * @example Experiment with concurrency control
   * ```typescript
   * const result = await langfuse.experiment.run({
   *   name: "Large Scale Evaluation",
   *   data: largeBatchOfItems,
   *   task: expensiveModelCall,
   *   maxConcurrency: 5, // Process max 5 items simultaneously
   *   evaluators: [myEvaluator],
   *   runEvaluators: [
   *     async ({ itemResults }) => ({
   *       name: "average_score",
   *       value: itemResults.reduce((acc, r) => acc + r.evaluations[0].value, 0) / itemResults.length
   *     })
   *   ]
   * });
   * ```
   *
   * @see {@link ExperimentParams} for detailed parameter documentation
   * @see {@link ExperimentResult} for detailed return value documentation
   * @see {@link Evaluator} for evaluator function specifications
   * @see {@link RunEvaluator} for run evaluator function specifications
   *
   * @public
   */
  async run(config: ExperimentParams): Promise<ExperimentResult> {
    const {
      data,
      evaluators,
      task,
      name,
      description,
      metadata,
      maxConcurrency: batchSize = Infinity,
      runEvaluators,
    } = config;

    if (!this.isOtelRegistered()) {
      this.logger.warn(
        "OpenTelemetry has not been set up. Traces will not be sent to Langfuse.See our docs on how to set up OpenTelemetry: https://langfuse.com/docs/observability/sdk/typescript/setup#tracing-setup",
      );
    }

    const itemResults: ExperimentItemResult[] = [];

    for (let i = 0; i < data.length; i += batchSize) {
      const batch = data.slice(i, i + batchSize);

      const promises: Promise<ExperimentItemResult>[] = batch.map(
        async (item) => {
          return this.runItem({
            item,
            evaluators,
            task,
            experimentName: name,
            experimentDescription: description,
            experimentMetadata: metadata,
          });
        },
      );

      const results = await Promise.all(promises);

      itemResults.push(...results);
    }

    // Get dataset run URL
    const datasetRunId =
      itemResults.length > 0 ? itemResults[0].datasetRunId : undefined;

    let datasetRunUrl = undefined;
    if (datasetRunId && data.length > 0 && "datasetId" in data[0]) {
      const datasetId = data[0].datasetId;
      const projectUrl = (await this.langfuseClient.getTraceUrl("mock")).split(
        "/traces",
      )[0];

      datasetRunUrl = `${projectUrl}/datasets/${datasetId}/runs/${datasetRunId}`;
    }

    // Execute run evaluators
    let runEvaluations: Evaluation[] = [];
    if (runEvaluators && runEvaluators?.length > 0) {
      const promises = runEvaluators.map(async (runEvaluator) => {
        return runEvaluator({ itemResults })
          .then((result) => {
            // Handle both single evaluation and array of evaluations
            return Array.isArray(result) ? result : [result];
          })
          .catch((err) => {
            this.logger.error("Run evaluator failed with error ", err);

            throw err;
          });
      });

      runEvaluations = (await Promise.allSettled(promises)).reduce(
        (acc, settledPromise) => {
          if (settledPromise.status === "fulfilled") {
            acc.push(...settledPromise.value);
          }

          return acc;
        },
        [] as Evaluation[],
      );

      if (datasetRunId) {
        runEvaluations.forEach((runEval) =>
          this.langfuseClient.score.create({ datasetRunId, ...runEval }),
        );
      }
    }

    await this.langfuseClient.score.flush();

    return {
      itemResults,
      datasetRunId,
      runEvaluations,
      prettyPrint: async (options?: { includeItemResults?: boolean }) =>
        await this.prettyPrintResults({
          datasetRunUrl,
          itemResults,
          originalData: data,
          runEvaluations,
          name: config.name,
          description: config.description,
          includeItemResults: options?.includeItemResults ?? true,
        }),
    };
  }

  /**
   * Executes the task and evaluators for a single data item.
   *
   * This method handles the complete processing pipeline for one data item:
   * 1. Executes the task within a traced observation span
   * 2. Links the result to a dataset run (if applicable)
   * 3. Runs all item-level evaluators on the output
   * 4. Stores evaluation scores in Langfuse
   * 5. Handles errors gracefully by continuing with remaining evaluators
   *
   * @param params - Parameters for item execution
   * @param params.experimentName - Name of the parent experiment
   * @param params.experimentDescription - Description of the parent experiment
   * @param params.experimentMetadata - Metadata for the parent experiment
   * @param params.item - The data item to process
   * @param params.task - The task function to execute
   * @param params.evaluators - Optional evaluators to run on the output
   *
   * @returns Promise resolving to the item result with output, evaluations, and trace info
   *
   * @throws {Error} When task execution fails (propagated from task function)
   *
   * @internal
   */
  private async runItem(params: {
    experimentName: ExperimentParams["name"];
    experimentDescription: ExperimentParams["description"];
    experimentMetadata: ExperimentParams["metadata"];
    item: ExperimentParams["data"][0];
    task: ExperimentTask;
    evaluators?: Evaluator[];
  }): Promise<ExperimentItemResult> {
    const { item, evaluators = [], task } = params;

    const { output, traceId } = await startActiveObservation(
      "experiment-item-run",
      async (span) => {
        const output = await task(item);

        span.update({
          input: item.input,
          output,
        });

        return { output, traceId: span.traceId };
      },
    );

    let datasetRunId: string | undefined = undefined;

    if ("id" in item) {
      await this.langfuseClient.api.datasetRunItems
        .create({
          runName: params.experimentName,
          runDescription: params.experimentDescription,
          metadata: params.experimentMetadata,
          datasetItemId: item.id,
          traceId,
        })
        .then((result) => {
          datasetRunId = result.datasetRunId;
        })
        .catch((err) =>
          this.logger.error("Linking dataset run item failed", err),
        );
    }

    const evalPromises: Promise<Evaluation[]>[] = evaluators.map(
      async (evaluator) => {
        const params = {
          input: item.input,
          expectedOutput: item.expectedOutput,
          output,
        };

        return evaluator(params)
          .then((result) => {
            // Handle both single evaluation and array of evaluations
            return Array.isArray(result) ? result : [result];
          })
          .catch((err) => {
            this.logger.error(
              `Evaluator '${evaluator.name}' failed for params \n\n${JSON.stringify(params)}\n\n with error: ${err}`,
            );

            throw err;
          });
      },
    );

    const evals = (await Promise.allSettled(evalPromises)).reduce(
      (acc, promiseResult) => {
        if (promiseResult.status === "fulfilled") {
          acc.push(...promiseResult.value.flat());
        }

        return acc;
      },
      [] as Evaluation[],
    );

    for (const ev of evals) {
      this.langfuseClient.score.create({
        traceId,
        name: ev.name,
        comment: ev.comment,
        value: ev.value,
        metadata: ev.metadata,
        dataType: ev.dataType,
      });
    }

    return {
      output,
      evaluations: evals,
      traceId,
      datasetRunId,
    };
  }

  /**
   * Formats experiment results into a human-readable string representation.
   *
   * Creates a comprehensive, nicely formatted summary of the experiment including:
   * - Individual item results with inputs, outputs, expected values, and scores
   * - Dataset item and trace links (when available)
   * - Experiment overview with aggregate statistics
   * - Average scores across all evaluations
   * - Run-level evaluation results
   * - Links to dataset runs in the Langfuse UI
   *
   * @param params - Formatting parameters
   * @param params.datasetRunUrl - Optional URL to the dataset run in Langfuse UI
   * @param params.itemResults - Results from processing each data item
   * @param params.originalData - The original input data items
   * @param params.runEvaluations - Results from run-level evaluators
   * @param params.name - Name of the experiment
   * @param params.description - Optional description of the experiment
   * @param params.includeItemResults - Whether to include individual item details (default: true)
   *
   * @returns Promise resolving to formatted string representation
   *
   * @example Output format
   * ```
   * 1. Item 1:
   *    Input:    What is the capital of France?
   *    Expected: Paris
   *    Actual:   Paris
   *    Scores:
   *      â€¢ exact_match: 1.000
   *      â€¢ similarity: 0.95
   *        ðŸ’­ Very close match with expected output
   *
   *    Dataset Item:
   *    https://cloud.langfuse.com/project/123/datasets/456/items/789
   *
   *    Trace:
   *    https://cloud.langfuse.com/project/123/traces/abc123
   *
   * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   * ðŸ“Š Translation Quality Test - Testing model accuracy
   * 2 items
   * Evaluations:
   *   â€¢ exact_match
   *   â€¢ similarity
   *
   * Average Scores:
   *   â€¢ exact_match: 0.850
   *   â€¢ similarity: 0.923
   *
   * Run Evaluations:
   *   â€¢ overall_quality: 0.887
   *     ðŸ’­ Good performance with room for improvement
   *
   * ðŸ”— Dataset Run:
   *    https://cloud.langfuse.com/project/123/datasets/456/runs/def456
   * ```
   *
   * @internal
   */
  private async prettyPrintResults(params: {
    datasetRunUrl?: string;
    itemResults: ExperimentItemResult[];
    originalData: ExperimentItem[] | DatasetItem[];
    runEvaluations: Evaluation[];
    name: string;
    description?: string;
    includeItemResults?: boolean;
  }): Promise<string> {
    const {
      itemResults,
      originalData,
      runEvaluations,
      name,
      description,
      includeItemResults = true,
    } = params;

    if (itemResults.length === 0) {
      return "No experiment results to display.";
    }

    let output = "";

    // Individual results
    if (includeItemResults) {
      for (let index = 0; index < itemResults.length; index++) {
        const result = itemResults[index];
        const originalItem = originalData[index];

        output += `\n${index + 1}. Item ${index + 1}:\n`;

        // Input, expected, and actual on separate lines
        if (originalItem?.input !== undefined) {
          output += `   Input:    ${this.formatValue(originalItem.input)}\n`;
        }

        const expectedOutput =
          originalItem?.expectedOutput ?? result.expectedOutput ?? null;
        output += `   Expected: ${expectedOutput !== null ? this.formatValue(expectedOutput) : "null"}\n`;
        output += `   Actual:   ${this.formatValue(result.output)}\n`;

        // Scores on separate lines
        if (result.evaluations.length > 0) {
          output += `   Scores:\n`;
          result.evaluations.forEach((evaluation) => {
            const score =
              typeof evaluation.value === "number"
                ? evaluation.value.toFixed(3)
                : evaluation.value;
            output += `     â€¢ ${evaluation.name}: ${score}`;
            if (evaluation.comment) {
              output += `\n       ðŸ’­ ${evaluation.comment}`;
            }
            output += "\n";
          });
        }

        // Dataset item link on separate line
        if (
          originalItem &&
          "id" in originalItem &&
          "datasetId" in originalItem
        ) {
          const projectUrl = (
            await this.langfuseClient.getTraceUrl("mock")
          ).split("/traces")[0];
          const datasetItemUrl = `${projectUrl}/datasets/${originalItem.datasetId}/items/${originalItem.id}`;
          output += `\n   Dataset Item:\n   ${datasetItemUrl}\n`;
        }

        // Trace link on separate line
        if (result.traceId) {
          const traceUrl = await this.langfuseClient.getTraceUrl(
            result.traceId,
          );
          output += `\n   Trace:\n   ${traceUrl}\n`;
        }
      }
    } else {
      output += `Individual Results: Hidden (${itemResults.length} items)\n`;
      output +=
        "ðŸ’¡ Call prettyPrint({ includeItemResults: true }) to view them\n";
    }

    // Experiment Overview
    const totalItems = itemResults.length;
    const evaluationNames = new Set(
      itemResults.flatMap((r) => r.evaluations.map((e) => e.name)),
    );

    output += `\n${"â”€".repeat(50)}\n`;
    output += `ðŸ“Š ${name}`;
    if (description) {
      output += ` - ${description}`;
    }

    output += `\n${totalItems} items`;

    if (evaluationNames.size > 0) {
      output += `\nEvaluations:`;
      Array.from(evaluationNames).forEach((evalName) => {
        output += `\n  â€¢ ${evalName}`;
      });
      output += "\n";
    }

    // Average scores in bulleted list
    if (evaluationNames.size > 0) {
      output += `\nAverage Scores:`;
      for (const evalName of evaluationNames) {
        const scores = itemResults
          .flatMap((r) => r.evaluations)
          .filter((e) => e.name === evalName && typeof e.value === "number")
          .map((e) => e.value as number);

        if (scores.length > 0) {
          const avg = scores.reduce((a, b) => a + b, 0) / scores.length;
          output += `\n  â€¢ ${evalName}: ${avg.toFixed(3)}`;
        }
      }
      output += "\n";
    }

    // Run evaluations
    if (runEvaluations.length > 0) {
      output += `\nRun Evaluations:`;
      runEvaluations.forEach((runEval) => {
        const score =
          typeof runEval.value === "number"
            ? runEval.value.toFixed(3)
            : runEval.value;
        output += `\n  â€¢ ${runEval.name}: ${score}`;
        if (runEval.comment) {
          output += `\n    ðŸ’­ ${runEval.comment}`;
        }
      });
      output += "\n";
    }

    if (params.datasetRunUrl) {
      output += `\nðŸ”— Dataset Run:\n   ${params.datasetRunUrl}`;
    }

    return output;
  }

  /**
   * Formats a value for display in pretty-printed output.
   *
   * Handles different value types appropriately:
   * - Strings: Truncates long strings to 50 characters with "..."
   * - Objects/Arrays: Converts to JSON string representation
   * - Primitives: Uses toString() representation
   *
   * @param value - The value to format
   * @returns Formatted string representation suitable for display
   *
   * @internal
   */
  private formatValue(value: any): string {
    if (typeof value === "string") {
      return value.length > 50 ? `${value.substring(0, 47)}...` : value;
    }
    return JSON.stringify(value);
  }

  private isOtelRegistered(): boolean {
    let tracerProvider = trace.getTracerProvider();

    if (tracerProvider instanceof ProxyTracerProvider) {
      tracerProvider = tracerProvider.getDelegate();
    }

    return tracerProvider.constructor.name !== "NoopTracerProvider";
  }
}
