import { describe, it, beforeEach, afterEach } from "vitest";
import {
  startObservation,
  startActiveObservation,
  observe,
} from "@langfuse/tracing";
import {
  setupServerTestEnvironment,
  teardownServerTestEnvironment,
  waitForServerIngestion,
  type ServerTestEnvironment,
} from "./helpers/serverSetup.js";
import { ServerAssertions } from "./helpers/serverAssertions.js";
import { nanoid } from "nanoid";

describe("Server Export E2E Tests", () => {
  let testEnv: ServerTestEnvironment;
  let assertions: ServerAssertions;

  beforeEach(async () => {
    testEnv = await setupServerTestEnvironment();
    assertions = new ServerAssertions();
  });

  afterEach(async () => {
    await teardownServerTestEnvironment(testEnv);
  });

  it("should export span with nested generation to Langfuse server", async () => {
    const testId = nanoid(8);
    const traceName = `e2e-test-trace-${testId}`;
    const parentSpanName = `e2e-parent-span-${testId}`;
    const generationName = `nested-llm-call-${testId}`;

    // Create a parent span with trace metadata
    const parentSpan = startObservation(parentSpanName, {
      input: { operation: "E2E test operation" },
      metadata: { testType: "e2e", timestamp: Date.now() },
    });

    // Set trace attributes
    parentSpan.updateTrace({
      name: traceName,
      userId: "test-user-123",
      sessionId: "test-session-456",
      tags: ["e2e", "test"],
      public: true,
      metadata: { testRun: "server-export", version: "1.0.0" },
    });

    // Create a nested generation using the parent span
    const generation = parentSpan.startObservation(
      generationName,
      {
        model: "gpt-4",
        input: {
          messages: [{ role: "user", content: "What is OpenTelemetry?" }],
        },
        metadata: { temperature: 0.7, maxTokens: 100 },
      },
      { asType: "generation" },
    );

    // Simulate LLM response
    generation.update({
      output: {
        role: "assistant",
        content: "OpenTelemetry is an observability framework...",
      },
      usageDetails: {
        prompt_tokens: 15,
        completion_tokens: 25,
        total_tokens: 40,
      },
    });

    // Complete the generation and parent span
    generation.end();
    parentSpan.update({ output: { status: "completed", generationCount: 1 } });
    parentSpan.end();

    // Force flush to send spans to server
    await testEnv.spanProcessor.forceFlush();

    // Wait for server-side async ingestion processing
    await waitForServerIngestion(2000);

    // Fetch the trace from Langfuse server and verify
    const traces = await assertions.fetchTraces({
      name: traceName,
      limit: 1,
    });

    if (traces.length === 0) {
      throw new Error(`No traces found with name '${traceName}'`);
    }

    // Fetch the full trace with observations
    const trace = await assertions.fetchTrace(traces[0].id);

    // Assert trace properties
    assertions.expectTraceExists(trace, {
      name: traceName,
      userId: "test-user-123",
      sessionId: "test-session-456",
      public: true,
    });

    // Assert we have 2 observations (1 span + 1 generation)
    assertions.expectObservationCount(trace, 2);

    // Assert parent span exists with correct properties
    const parentObservation = assertions.expectObservationExists(
      trace,
      parentSpanName,
      {
        type: "SPAN",
        level: "DEFAULT",
      },
    );

    // Assert nested generation exists with correct properties
    const generationObservation = assertions.expectObservationExists(
      trace,
      generationName,
      {
        type: "GENERATION",
        model: "gpt-4",
        level: "DEFAULT",
      },
    );

    // Assert parent-child relationship
    assertions.expectObservationParent(trace, generationName, parentSpanName);

    // Assert generation has usage data
    if (!generationObservation.usage) {
      throw new Error("Generation observation missing usage data");
    }

    if (generationObservation.usage.total !== 40) {
      throw new Error(
        `Expected total tokens 40, got ${generationObservation.usage.total}`,
      );
    }

    console.log(
      "âœ… E2E test passed: Trace exported successfully to Langfuse server",
    );
    console.log(`ðŸ“Š Trace ID: ${trace.id}`);
    console.log(`ðŸ“ˆ Observations: ${trace.observations.length}`);
  });

  it("should export startActiveObservation with nested startActiveObservation generation to Langfuse server", async () => {
    const testId = nanoid(8);
    const traceName = `e2e-active-span-trace-${testId}`;
    const parentSpanName = `active-parent-operation-${testId}`;
    const generationName = `nested-active-generation-${testId}`;

    // Use startActiveObservation with automatic context management
    const result = await startActiveObservation(
      parentSpanName,
      async (parentSpan) => {
        // Set trace attributes
        parentSpan.updateTrace({
          name: traceName,
          userId: "active-user-789",
          sessionId: "active-session-012",
          tags: ["active", "e2e"],
          metadata: { testType: "activeSpan", framework: "vitest" },
        });

        // Update parent span
        parentSpan.update({
          input: { workflow: "active span testing" },
          metadata: { step: "parent", priority: "high" },
        });

        // Use startActiveObservation with generation type within the active span context
        const generationResult = await startActiveObservation(
          generationName,
          async (generation) => {
            // This generation should automatically be nested under the active span
            generation.update({
              model: "gpt-3.5-turbo",
              input: {
                messages: [
                  { role: "system", content: "You are a helpful assistant" },
                  { role: "user", content: "Explain active spans" },
                ],
              },
              metadata: { temperature: 0.5, maxTokens: 150 },
            });

            // Simulate some processing
            await new Promise((resolve) => setTimeout(resolve, 10));

            // Update with response
            generation.update({
              output: {
                role: "assistant",
                content: "Active spans provide automatic context management...",
              },
              usageDetails: {
                prompt_tokens: 25,
                completion_tokens: 35,
                total_tokens: 60,
              },
              level: "DEFAULT",
            });

            return "generation-completed";
          },
          { asType: "generation" },
        );

        // Update parent span with final results
        parentSpan.update({
          output: {
            workflow: "completed",
            generationResult,
            totalOperations: 1,
          },
        });

        return "parent-operation-completed";
      },
    );

    // Force flush and wait for ingestion
    await testEnv.spanProcessor.forceFlush();
    await waitForServerIngestion(2000);

    // Verify the trace
    const traces = await assertions.fetchTraces({
      name: traceName,
      limit: 1,
    });

    if (traces.length === 0) {
      throw new Error(`No traces found with name '${traceName}'`);
    }

    // Fetch the full trace with observations
    const trace = await assertions.fetchTrace(traces[0].id);

    // Assert trace properties
    assertions.expectTraceExists(trace, {
      name: traceName,
      userId: "active-user-789",
      sessionId: "active-session-012",
    });

    // Should have 2 observations: 1 span + 1 generation
    assertions.expectObservationCount(trace, 2);

    // Verify parent span
    assertions.expectObservationExists(trace, parentSpanName, {
      type: "SPAN",
      level: "DEFAULT",
    });

    // Verify nested generation
    const generationObs = assertions.expectObservationExists(
      trace,
      generationName,
      {
        type: "GENERATION",
        model: "gpt-3.5-turbo",
        level: "DEFAULT",
      },
    );

    // Verify parent-child relationship
    assertions.expectObservationParent(trace, generationName, parentSpanName);

    // Verify usage data
    if (generationObs.usage?.total !== 60) {
      throw new Error(
        `Expected total tokens 60, got ${generationObs.usage?.total}`,
      );
    }

    console.log("âœ… Active span/generation test passed");
    console.log(`ðŸ“Š Trace ID: ${trace.id}`);
  });

  it("should export observe wrapper with interoperability to Langfuse server", async () => {
    const testId = nanoid(8);
    const traceName = `e2e-observe-interop-trace-${testId}`;
    const coordinatorSpanName = `workflow-coordinator-${testId}`;
    const observedSpanName = `observed-llm-workflow-${testId}`;
    const internalGenerationName = `internal-llm-generation-${testId}`;
    const postProcessingName = `post-processing-${testId}`;
    const finalGenerationName = `final-summary-${testId}`;

    // Create an observed function that uses other tracing methods
    const observedLLMCall = observe(
      async (prompt: string, options: { temperature: number }) => {
        // This function will be automatically wrapped in a span
        console.log(`Processing prompt: ${prompt}`);

        // Use startActiveObservation with generation type within the observed function
        const response = await startActiveObservation(
          internalGenerationName,
          async (generation) => {
            generation.update({
              model: "claude-3-sonnet",
              input: [{ role: "user", content: prompt }],
              metadata: { ...options, source: "observed-function" },
            });

            // Simulate LLM processing
            await new Promise((resolve) => setTimeout(resolve, 15));

            const responseContent = `Response to: ${prompt}`;
            const tokens = Math.floor(Math.random() * 100) + 50;

            generation.update({
              output: {
                role: "assistant",
                content: responseContent,
              },
              usageDetails: {
                prompt_tokens: prompt.length / 4,
                completion_tokens: tokens,
                total_tokens: prompt.length / 4 + tokens,
              },
            });

            const result = {
              content: responseContent,
              tokens,
            };

            return result;
          },
          { asType: "generation" },
        );

        // Use manual span creation within observed function
        const processingSpan = startObservation(postProcessingName, {
          input: { response },
          metadata: { stage: "post-processing" },
        });

        // Simulate post-processing
        await new Promise((resolve) => setTimeout(resolve, 5));

        const finalResult = {
          ...response,
          processed: true,
          timestamp: Date.now(),
        };

        processingSpan.update({ output: finalResult });
        processingSpan.end();

        return finalResult;
      },
      {
        name: observedSpanName,
        asType: "span",
        captureInput: true,
        captureOutput: true,
      },
    );

    // Use the observed function within an active span context
    const workflowResult = await startActiveObservation(
      coordinatorSpanName,
      async (coordinatorSpan) => {
        coordinatorSpan.updateTrace({
          name: traceName,
          userId: "observe-user-456",
          sessionId: "observe-session-789",
          tags: ["observe", "interop", "e2e"],
          metadata: { testType: "observe-interop", complexity: "high" },
        });

        coordinatorSpan.update({
          input: { workflow: "multi-method tracing test" },
          metadata: {
            coordinator: true,
            methods: ["observe", "active", "manual"],
          },
        });

        // Call the observed function (which internally uses other tracing methods)
        const llmResult = await observedLLMCall(
          "What is the meaning of life?",
          {
            temperature: 0.7,
          },
        );

        // Create a manual generation in the same context
        const finalGeneration = coordinatorSpan.startObservation(
          finalGenerationName,
          {
            model: "gpt-4",
            input: [
              {
                role: "user",
                content: `Summarize this workflow result: ${JSON.stringify(llmResult)}`,
              },
            ],
            metadata: { type: "summary", final: true },
          },
          { asType: "generation" },
        );

        finalGeneration.update({
          output: {
            role: "assistant",
            content:
              "Workflow completed successfully with 3 total steps using methods: observe, startActiveGeneration, startSpan, manual",
          },
          usageDetails: {
            prompt_tokens: 10,
            completion_tokens: 15,
            total_tokens: 25,
          },
        });
        finalGeneration.end();

        coordinatorSpan.update({
          output: {
            status: "completed",
            llmResult,
            totalObservations: 4,
          },
        });

        return { llmResult, status: "success" };
      },
    );

    // Force flush and wait for ingestion
    await testEnv.spanProcessor.forceFlush();
    await waitForServerIngestion(2000);

    // Verify the complex trace
    const traces = await assertions.fetchTraces({
      name: traceName,
      limit: 1,
    });

    if (traces.length === 0) {
      throw new Error(`No traces found with name '${traceName}'`);
    }

    // Fetch the full trace with observations
    const trace = await assertions.fetchTrace(traces[0].id);

    // Assert trace properties
    assertions.expectTraceExists(trace, {
      name: traceName,
      userId: "observe-user-456",
      sessionId: "observe-session-789",
    });

    // Should have 5 observations:
    // 1. workflow-coordinator (span)
    // 2. observed-llm-workflow (span from observe)
    // 3. internal-llm-generation (generation from startActiveGeneration)
    // 4. post-processing (span from manual startSpan)
    // 5. final-summary (generation from manual)
    assertions.expectObservationCount(trace, 5);

    // Verify all observations exist
    assertions.expectObservationExists(trace, coordinatorSpanName, {
      type: "SPAN",
    });
    assertions.expectObservationExists(trace, observedSpanName, {
      type: "SPAN",
    });
    assertions.expectObservationExists(trace, internalGenerationName, {
      type: "GENERATION",
      model: "claude-3-sonnet",
    });
    assertions.expectObservationExists(trace, postProcessingName, {
      type: "SPAN",
    });
    assertions.expectObservationExists(trace, finalGenerationName, {
      type: "GENERATION",
      model: "gpt-4",
    });

    // Verify key parent-child relationships
    assertions.expectObservationParent(
      trace,
      observedSpanName,
      coordinatorSpanName,
    );
    assertions.expectObservationParent(
      trace,
      internalGenerationName,
      observedSpanName,
    );
    assertions.expectObservationParent(
      trace,
      postProcessingName,
      observedSpanName,
    );
    assertions.expectObservationParent(
      trace,
      finalGenerationName,
      coordinatorSpanName,
    );

    console.log("âœ… Observe interoperability test passed");
    console.log(`ðŸ“Š Trace ID: ${trace.id}`);
    console.log(
      `ðŸ”— Complex nesting with ${trace.observations.length} observations`,
    );
  });

  it("should export spans media handling to Langfuse server", async () => {
    const testId = nanoid(8);
    const traceName = `e2e-masking-media-trace-${testId}`;
    const coordinatorSpanName = `media-masking-workflow-${testId}`;
    const visionGenerationName = `vision-analysis-${testId}`;
    const fileProcessingName = `file-processing-${testId}`;
    const workflowStartedEventName = `workflow-started-${testId}`;
    const analysisCompletedEventName = `analysis-completed-${testId}`;
    const fileStartEventName = `file-processing-started-${testId}`;
    const fileCompleteEventName = `file-processing-completed-${testId}`;
    const workflowCompleteEventName = `workflow-completed-${testId}`;

    // Create base64 image data for media testing
    const base64Image =
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==";
    const base64Audio =
      "data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQAAAAA=";

    // Update server test environment to use masking
    await testEnv.shutdown();
    testEnv = await setupServerTestEnvironment();

    const workflowResult = await startActiveObservation(
      coordinatorSpanName,
      async (coordinatorSpan) => {
        coordinatorSpan.updateTrace({
          name: traceName,
          userId: "secure-user-123",
          sessionId: "secure-session-456",
          tags: ["masking", "media", "security", "e2e", "comprehensive"],
          public: true,
          version: "1.2.0",
          release: "e2e-test-release",
          environment: "test",
          input: {
            workflowType: "comprehensive-testing",
            securityLevel: "enterprise",
          },
          output: {
            status: "initializing",
          },
          metadata: {
            securityLevel: "high",
          },
        });

        coordinatorSpan.update({
          input: {
            workflow: "media processing",
          },
          output: {
            status: "processing",
          },
          metadata: {
            priority: "high",
          },
          level: "DEFAULT",
          statusMessage: "Coordinator span initialized successfully",
        });

        // Create startup event
        const startupEvent = coordinatorSpan.startObservation(
          workflowStartedEventName,
          {
            input: {
              initiator: "e2e-test",
            },
            metadata: {
              eventType: "lifecycle",
              importance: "high",
            },
            level: "DEFAULT",
            statusMessage: "Workflow startup event triggered",
          },
          { asType: "event" },
        );

        // Create a generation with media content
        const mediaGeneration = await startActiveObservation(
          visionGenerationName,
          async (generation) => {
            generation.update({
              model: "gpt-4-vision-preview",
              modelParameters: {
                temperature: 0.3,
                max_tokens: 500,
                top_p: 0.9,
                frequency_penalty: 0.1,
                presence_penalty: 0.2,
              },
              input: [
                {
                  role: "system",
                  content: "You are an expert multimedia analyst.",
                },
                {
                  role: "user",
                  content: [
                    { type: "text", text: "Analyze this image and audio:" },
                    { type: "image_url", image_url: { url: base64Image } },
                    { type: "audio", audio_data: base64Audio },
                  ],
                },
              ],
              metadata: {
                analysisType: "multimedia",
                generationType: "vision-analysis",
                version: "1.5.0",
              },
              version: "1.5.0",
              prompt: {
                name: "summary-prompt",
                version: 1,
              },
            });

            // Create completion event
            const completionEvent = generation.startObservation(
              analysisCompletedEventName,
              {
                output: {
                  processingTime: 20,
                  mediaItemsProcessed: 2,
                  confidence: 0.95,
                },
                metadata: {
                  step: "analysis-completion",
                  quality: "high",
                },
                level: "DEFAULT",
                statusMessage: "Multimedia analysis completed successfully",
              },
              { asType: "event" },
            );

            generation.update({
              output: {
                role: "assistant",
                content: `I've analyzed the multimedia content:

**Image Analysis:**
- Format: PNG (1x1 pixels)
- Type: Small transparent image
- Confidence: 95%

**Audio Analysis:**
- Format: WAV (0.1s duration)
- Type: Silent audio file
- Confidence: 95%

**Summary:**
Both media items were successfully processed. The image is a minimal transparent PNG and the audio is a brief silent WAV file. Processing completed in 20ms with high confidence scores.`,
              },
              metadata: {
                confidence: 0.95,
                qualityScore: 0.88,
                processingVersion: "2.1.0",
                // Detailed analysis results
                analysis: {
                  imageDescription: "Small transparent PNG image detected",
                  audioDescription: "Silent WAV audio file detected",
                  mediaProcessed: true,
                  confidence: 0.95,
                  processingTime: "20ms",
                  // Media should be converted to references
                  processedImage: base64Image,
                  processedAudio: base64Audio,
                  results: {
                    imageAnalysis: {
                      format: "PNG",
                      dimensions: "1x1",
                      transparency: true,
                    },
                    audioAnalysis: {
                      format: "WAV",
                      duration: "0.1s",
                      silence: true,
                    },
                  },
                },
                // More sensitive data
                processingSecret: "internal-processing-key-abc",
                outputSecret: "generation-output-secret",
              },
              usageDetails: {
                input: 150,
                output: 75,
                total: 225,
              },
              level: "DEFAULT",
              statusMessage: "Vision analysis completed with high confidence",
              completionStartTime: new Date(Date.now() - 20),
            });

            return "media-analysis-completed";
          },
          { asType: "generation" },
        );

        // Create a span with file processing simulation
        const fileProcessingSpan = startObservation(fileProcessingName, {
          input: {
            files: [
              {
                name: "document.pdf",
                size: 1024,
                mimeType: "application/pdf",
                content: base64Image, // Simulating file content
                metadata: {
                  uploadedBy: "user@secure.com",
                  uploadTimestamp: Date.now(),
                  // Should be masked
                  processingKey: "file-key-secret",
                  uploaderToken: "sk-uploader-token",
                },
              },
            ],
            // More sensitive configuration
            processingConfig: {
              apiEndpoint: "https://api.secure.com",
              authToken: "bearer-sk-secret-token",
              encryptionKey: "encryption-key-12345",
              retryAttempts: 3,
              timeout: 30000,
              enableCompression: true,
            },
            options: {
              extractText: true,
              generateThumbnail: true,
              performOCR: true,
              qualityCheck: true,
            },
          },
          output: {
            status: "starting",
            queuePosition: 1,
          },
          metadata: {
            stage: "file-processing",
            securityScan: true,
            processingVersion: "3.2.1",
            spanType: "file-processor",
          },
          level: "DEFAULT",
          statusMessage: "File processing span initialized",
          version: "3.2.1",
        });

        const fileStartEvent = fileProcessingSpan.startObservation(
          fileStartEventName,
          {
            input: {
              fileName: "document.pdf",
              fileSize: 1024,
              processingMode: "enhanced",
            },
            metadata: {
              eventType: "file-lifecycle",
              processor: "enhanced-pdf",
            },
            level: "DEFAULT",
            statusMessage: "Started processing document.pdf",
          },
          { asType: "event" },
        );

        // Simulate file processing
        await new Promise((resolve) => setTimeout(resolve, 15));

        // Create file processing completion event
        const fileCompleteEvent = fileProcessingSpan.startObservation(
          fileCompleteEventName,
          {
            output: {
              fileName: "document.pdf",
              processingTime: 15,
              operationsPerformed: [
                "text-extraction",
                "thumbnail-generation",
                "ocr",
                "quality-check",
              ],
              success: true,
            },
            metadata: {
              eventType: "file-lifecycle",
              completionQuality: "high",
            },
            level: "DEFAULT",
            statusMessage: "File processing completed successfully",
          },
          { asType: "event" },
        );

        fileProcessingSpan.update({
          output: {
            processedFiles: [
              {
                name: "document.pdf",
                status: "processed",
                size: 1024,
                pages: 1,
                thumbnail: base64Image, // Should be converted to media reference
                extractedText: "Sample document content",
                ocrConfidence: 0.98,
                qualityScore: 0.95,
                processingTime: "15ms",
                operations: {
                  textExtraction: { success: true, confidence: 0.98 },
                  thumbnailGeneration: { success: true, size: "64x64" },
                  ocr: { success: true, language: "en", confidence: 0.98 },
                  qualityCheck: { success: true, score: 0.95 },
                },
                metadata: {
                  processor: "enhanced-pdf-v3.2.1",
                },
              },
            ],
            summary: {
              totalFiles: 1,
              successfulFiles: 1,
              failedFiles: 0,
              status: "completed",
              totalProcessingTime: "15ms",
              averageQuality: 0.95,
              operationsPerformed: 4,
            },
            performance: {
              throughput: "68.27 files/second",
              efficiency: 0.92,
              resourceUsage: "low",
            },
          },
          level: "DEFAULT",
          statusMessage: "File processing completed with high quality scores",
        });
        fileProcessingSpan.end();

        const startTime = Date.now() - 100; // Simulating start time

        // Create workflow completion event
        const workflowCompleteEvent = coordinatorSpan.startObservation(
          workflowCompleteEventName,
          {
            output: {
              totalDuration: Date.now() - startTime,
              operationsCompleted: 5,
              qualityScore: 0.94,
              success: true,
            },
            metadata: {
              eventType: "workflow-lifecycle",
              completionStatus: "success",
              performanceGrade: "A",
            },
            level: "DEFAULT",
            statusMessage: "Comprehensive workflow completed successfully",
          },
          { asType: "event" },
        );

        // Update trace output
        coordinatorSpan.updateTrace({
          output: {
            status: "completed",
            totalObservations: 8, // coordinator + generation + file span + 5 events
            mediaItemsProcessed: 4, // 2 in generation + 2 in file processing
            overallQuality: 0.94,
          },
        });

        coordinatorSpan.update({
          output: {
            status: "completed",
            mediaAnalysis: mediaGeneration,
            fileProcessingCompleted: true,
            totalOperations: 5,
            executionTime: "~55ms",
            qualityMetrics: {
              mediaAnalysisQuality: 0.95,
              fileProcessingQuality: 0.95,
              overallWorkflowQuality: 0.94,
            },
            results: {
              mediaAnalysisSuccess: true,
              fileProcessingSuccess: true,
              eventsGenerated: 4,
              dataSecurityCompliant: true,
            },
            performance: {
              efficiency: 0.93,
              resourceUsage: "optimal",
              cacheHitRate: 0.0, // No cache in test
            },
            // Final sensitive data
            workflowSecret: "workflow-completion-secret",
            finalReport: {
              userEmail: "admin@company.com",
              internalKey: "report-secret-key",
              reportSecret: "sk-final-report-secret",
            },
            security: {
              dataMasked: true,
              encryptionApplied: true,
              auditTrailGenerated: true,
              // More secrets
              securitySecret: "security-final-secret",
            },
          },
          level: "DEFAULT",
          statusMessage:
            "Comprehensive masking and media workflow completed successfully",
        });

        return {
          status: "success",
          mediaProcessed: true,
          totalObservations: 8,
          qualityScore: 0.94,
          executionTime: Date.now() - startTime,
        };
      },
    );

    // Force flush and wait for ingestion
    await testEnv.spanProcessor.forceFlush();
    await waitForServerIngestion(3000); // Longer wait for media processing

    // Verify the trace with masking and media handling
    const traces = await assertions.fetchTraces({
      name: traceName,
      limit: 1,
    });

    if (traces.length === 0) {
      throw new Error(`No traces found with name '${traceName}'`);
    }

    // Fetch the full trace with observations
    const trace = await assertions.fetchTrace(traces[0].id);

    // Assert trace properties
    assertions.expectTraceExists(trace, {
      name: traceName,
      userId: "secure-user-123",
      sessionId: "secure-session-456",
    });

    // Should have 8 observations: coordinator span + generation + file processing span + 5 events
    assertions.expectObservationCount(trace, 8);

    // Verify all observations exist
    const coordinatorObs = assertions.expectObservationExists(
      trace,
      coordinatorSpanName,
      {
        type: "SPAN",
      },
    );
    const generationObs = assertions.expectObservationExists(
      trace,
      visionGenerationName,
      {
        type: "GENERATION",
        model: "gpt-4-vision-preview",
      },
    );
    const fileProcessingObs = assertions.expectObservationExists(
      trace,
      fileProcessingName,
      {
        type: "SPAN",
      },
    );

    // Verify events exist
    assertions.expectObservationExists(trace, workflowStartedEventName, {
      type: "EVENT",
    });
    assertions.expectObservationExists(trace, analysisCompletedEventName, {
      type: "EVENT",
    });
    assertions.expectObservationExists(trace, fileStartEventName, {
      type: "EVENT",
    });
    assertions.expectObservationExists(trace, fileCompleteEventName, {
      type: "EVENT",
    });
    assertions.expectObservationExists(trace, workflowCompleteEventName, {
      type: "EVENT",
    });

    // Verify parent-child relationships
    assertions.expectObservationParent(
      trace,
      visionGenerationName,
      coordinatorSpanName,
    );
    assertions.expectObservationParent(
      trace,
      fileProcessingName,
      coordinatorSpanName,
    );

    // Verify event relationships
    assertions.expectObservationParent(
      trace,
      workflowStartedEventName,
      coordinatorSpanName,
    );
    assertions.expectObservationParent(
      trace,
      analysisCompletedEventName,
      visionGenerationName,
    );
    assertions.expectObservationParent(
      trace,
      fileStartEventName,
      fileProcessingName,
    );
    assertions.expectObservationParent(
      trace,
      fileCompleteEventName,
      fileProcessingName,
    );
    assertions.expectObservationParent(
      trace,
      workflowCompleteEventName,
      coordinatorSpanName,
    );

    // Verify masking worked by checking trace metadata
    if (trace.metadata) {
      const metadataStr = JSON.stringify(trace.metadata);
      if (
        metadataStr.includes("sk-1234567890abcdef") ||
        metadataStr.includes("user@example.com")
      ) {
        throw new Error(
          "Sensitive data in trace metadata was not properly masked",
        );
      }
      if (
        !metadataStr.includes("***MASKED***") &&
        !metadataStr.includes("***@")
      ) {
        console.warn(
          "Warning: Expected to see masked values in trace metadata",
        );
      }
    }

    // Check that media content was processed (should be converted to media references)
    if (generationObs.input) {
      const inputStr = JSON.stringify(generationObs.input);
      // Should not contain the full base64 data
      if (inputStr.includes("iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJ")) {
        console.warn(
          "Warning: Base64 image data might not have been converted to media reference",
        );
      }
      // Should contain media reference markers or be processed
      if (
        inputStr.includes("@@@langfuseMedia:") ||
        !inputStr.includes("data:image/png;base64,iVBORw0K")
      ) {
        console.log("âœ… Media content appears to be processed correctly");
      }
    }

    console.log("âœ… Masking and media handling test passed");
    console.log(`ðŸ“Š Trace ID: ${trace.id}`);
    console.log(`ðŸ”’ Security: Sensitive data masked`);
    console.log(`ðŸŽ­ Media: Base64 content processed`);
    console.log(`ðŸ”— Observations: ${trace.observations.length}`);
  });
});
